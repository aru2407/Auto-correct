{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install keras_nlp","metadata":{"execution":{"iopub.status.busy":"2023-08-16T12:47:06.744577Z","iopub.execute_input":"2023-08-16T12:47:06.744946Z","iopub.status.idle":"2023-08-16T12:47:06.750056Z","shell.execute_reply.started":"2023-08-16T12:47:06.744895Z","shell.execute_reply":"2023-08-16T12:47:06.748990Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport numpy as np\nimport keras\nfrom keras.layers import Dense, Input, TextVectorization\nfrom keras.models import Sequential\nfrom keras_nlp.layers import TokenAndPositionEmbedding, TransformerDecoder\nfrom keras_nlp.metrics import Perplexity\nfrom keras.callbacks import ReduceLROnPlateau,EarlyStopping\nfrom nltk.metrics.distance import edit_distance","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lY1z_q9brL95","outputId":"cc3668bf-31a1-4c17-b434-940a108aae10","execution":{"iopub.status.busy":"2023-08-16T12:47:06.757739Z","iopub.execute_input":"2023-08-16T12:47:06.758856Z","iopub.status.idle":"2023-08-16T12:47:10.915542Z","shell.execute_reply.started":"2023-08-16T12:47:06.758822Z","shell.execute_reply":"2023-08-16T12:47:10.914401Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"}]},{"cell_type":"code","source":"with open(\"/kaggle/input/textgen/text.txt\", \"r\") as f:\n    raw_text = f.readlines()","metadata":{"id":"SxvIVrOXzkAF","execution":{"iopub.status.busy":"2023-08-16T12:47:10.917807Z","iopub.execute_input":"2023-08-16T12:47:10.918680Z","iopub.status.idle":"2023-08-16T12:47:10.952939Z","shell.execute_reply.started":"2023-08-16T12:47:10.918640Z","shell.execute_reply":"2023-08-16T12:47:10.951959Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/textgen/words_alpha.txt\",\"r\") as f:\n    data = [word.strip() for word in f.readlines()]\ndictionary = dict(zip(data, range(len(data))))","metadata":{"execution":{"iopub.status.busy":"2023-08-16T12:47:10.954304Z","iopub.execute_input":"2023-08-16T12:47:10.954897Z","iopub.status.idle":"2023-08-16T12:47:11.167459Z","shell.execute_reply.started":"2023-08-16T12:47:10.954863Z","shell.execute_reply":"2023-08-16T12:47:11.166359Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def preprocessing_text(raw_text):\n    text = str(raw_text[3:]) #First 3 lines are header data, so I'm taking into account for the data 4th line onwards\n    text = re.sub(r'@@\\d+','',text) #Replacing text like @@1234\n    text = re.sub(r'#','',text) #Replacing # from text\n    text = re.sub(r\"@(\\s@)+\",'',text)\n    text = re.sub(r'\\(.*?\\)','',text)\n    text = re.sub(r'\\s+\\'s', '', text)\n    text = re.sub(r'<\\w+>','',text)\n    text = re.sub(r'</\\w+>','',text)\n    text = re.sub(r'\\*\\*(\\d+;\\d+;[A-Za-z])','',text) #**37;11433;TOOLONG\n    text = re.sub(r\"\\\\'\",\"\",text) #\\' \\' Nature \\' \\'\n    text = re.sub(r'#&[a-zA-Z]+;[a-zA-Z]+\\s*;','',text) #&amp;ndash ;\n    text = re.sub(r'&[a-zA-Z]*;','',text) #&amp;\n    text = re.sub(r\"\\n\",' ',text)\n\n    text = text.lower()\n    return text.split(\".\")\n","metadata":{"id":"tSJQv8Hmzj1a","execution":{"iopub.status.busy":"2023-08-16T12:47:11.170293Z","iopub.execute_input":"2023-08-16T12:47:11.170772Z","iopub.status.idle":"2023-08-16T12:47:11.186833Z","shell.execute_reply.started":"2023-08-16T12:47:11.170736Z","shell.execute_reply":"2023-08-16T12:47:11.185493Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"text = preprocessing_text(raw_text)\ntext = list(text[:50000])\n","metadata":{"id":"YCtoBOMazjm4","execution":{"iopub.status.busy":"2023-08-16T12:47:11.188670Z","iopub.execute_input":"2023-08-16T12:47:11.189087Z","iopub.status.idle":"2023-08-16T12:47:11.839950Z","shell.execute_reply.started":"2023-08-16T12:47:11.189054Z","shell.execute_reply":"2023-08-16T12:47:11.838898Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"maxlen = 50\nvectorize = TextVectorization(\n    output_mode=\"int\",\n    output_sequence_length=maxlen+1\n)\n\nvectorize.adapt(text)\nvocab = vectorize.get_vocabulary()","metadata":{"id":"ywVB6zPAzrl7","execution":{"iopub.status.busy":"2023-08-16T12:47:11.841380Z","iopub.execute_input":"2023-08-16T12:47:11.841846Z","iopub.status.idle":"2023-08-16T12:47:21.103618Z","shell.execute_reply.started":"2023-08-16T12:47:11.841812Z","shell.execute_reply":"2023-08-16T12:47:21.102485Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"vocab_list = dict(zip(range(len(vocab)), vocab))\nvocab_size = len(vocab_list)\nprint(\"Total number of unique words found from the data: \", vocab_size)","metadata":{"id":"G5hFCSNRzrf2","execution":{"iopub.status.busy":"2023-08-16T12:47:21.105002Z","iopub.execute_input":"2023-08-16T12:47:21.105732Z","iopub.status.idle":"2023-08-16T12:47:21.120832Z","shell.execute_reply.started":"2023-08-16T12:47:21.105696Z","shell.execute_reply":"2023-08-16T12:47:21.119788Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Total number of unique words found from the data:  69598\n","output_type":"stream"}]},{"cell_type":"code","source":"train = text[:int(0.85*len(text))]\ntest = text[int(0.85*len(text)):]\n\ntrain = tf.data.Dataset.from_tensor_slices(train)\ntest = tf.data.Dataset.from_tensor_slices(test)\ntrain = train.batch(128)\ntest = test.batch(128)","metadata":{"id":"CxvT7kmbzrif","execution":{"iopub.status.busy":"2023-08-16T12:47:21.122211Z","iopub.execute_input":"2023-08-16T12:47:21.125270Z","iopub.status.idle":"2023-08-16T12:47:21.330339Z","shell.execute_reply.started":"2023-08-16T12:47:21.125232Z","shell.execute_reply":"2023-08-16T12:47:21.329331Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def vectorize_dataset(text):\n\n  text = tf.expand_dims(text, -1)\n  tokenized_text = vectorize(text)\n\n  x = tokenized_text[:, :-1]\n  y = tokenized_text[:, 1:]\n  return x,y\n\ntrain = train.map(vectorize_dataset)\ntrain = train.prefetch(tf.data.AUTOTUNE)\n#train = train.batch(1000)\n\ntest = test.map(vectorize_dataset)\ntest = test.prefetch(tf.data.AUTOTUNE)\n#test = test.batch(1000)","metadata":{"id":"6ug7T5WTtX0K","execution":{"iopub.status.busy":"2023-08-16T12:47:21.331703Z","iopub.execute_input":"2023-08-16T12:47:21.332087Z","iopub.status.idle":"2023-08-16T12:47:21.486121Z","shell.execute_reply.started":"2023-08-16T12:47:21.332053Z","shell.execute_reply":"2023-08-16T12:47:21.485090Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"input_layer = Input(shape=(maxlen,), dtype = tf.int32)\nembedding_layer = TokenAndPositionEmbedding(vocab_size, maxlen, 128)(input_layer)\ndecoder_layer = TransformerDecoder(\n    intermediate_dim = 128,\n    num_heads = 4,\n    dropout=0.2,\n    activation=\"relu\")(embedding_layer)\noutput_layer = Dense(vocab_size,\n                     activation=\"softmax\")(decoder_layer)\n\nmodel = keras.Model(inputs = input_layer, outputs = output_layer)\nmodel.summary()","metadata":{"id":"5O1q001h8Kbr","execution":{"iopub.status.busy":"2023-08-16T12:47:21.489766Z","iopub.execute_input":"2023-08-16T12:47:21.490073Z","iopub.status.idle":"2023-08-16T12:47:23.897507Z","shell.execute_reply.started":"2023-08-16T12:47:21.490046Z","shell.execute_reply":"2023-08-16T12:47:23.896602Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 50)]              0         \n                                                                 \n token_and_position_embeddin  (None, 50, 128)          8914944   \n g (TokenAndPositionEmbeddin                                     \n g)                                                              \n                                                                 \n transformer_decoder (Transf  (None, 50, 128)          99584     \n ormerDecoder)                                                   \n                                                                 \n dense_2 (Dense)             (None, 50, 69598)         8978142   \n                                                                 \n=================================================================\nTotal params: 17,992,670\nTrainable params: 17,992,670\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n        optimizer=\"adam\",\n        loss='sparse_categorical_crossentropy',\n        metrics=[Perplexity(), 'accuracy']\n    )","metadata":{"id":"SfszU0Sc8JcL","execution":{"iopub.status.busy":"2023-08-16T12:47:23.898569Z","iopub.execute_input":"2023-08-16T12:47:23.898949Z","iopub.status.idle":"2023-08-16T12:47:23.928897Z","shell.execute_reply.started":"2023-08-16T12:47:23.898896Z","shell.execute_reply":"2023-08-16T12:47:23.927837Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\nearly_stopping = EarlyStopping(monitor=\"val_accuracy\", patience=5, verbose=1)\nmodel.fit(train, validation_data = test, batch_size=2500, epochs=20, verbose=1,callbacks=[reduce_lr,early_stopping])","metadata":{"id":"ZeA-yA-D-W4T","execution":{"iopub.status.busy":"2023-08-16T12:48:37.064849Z","iopub.execute_input":"2023-08-16T12:48:37.065253Z","iopub.status.idle":"2023-08-16T13:14:16.918727Z","shell.execute_reply.started":"2023-08-16T12:48:37.065225Z","shell.execute_reply":"2023-08-16T13:14:16.917683Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/20\n333/333 [==============================] - 170s 512ms/step - loss: 2.6740 - perplexity: 14.4984 - accuracy: 0.6798 - val_loss: 2.6043 - val_perplexity: 13.5219 - val_accuracy: 0.6941 - lr: 0.0010\nEpoch 2/20\n333/333 [==============================] - 156s 468ms/step - loss: 2.4203 - perplexity: 11.2494 - accuracy: 0.6947 - val_loss: 2.5607 - val_perplexity: 12.9447 - val_accuracy: 0.6981 - lr: 0.0010\nEpoch 3/20\n333/333 [==============================] - 156s 468ms/step - loss: 2.2305 - perplexity: 9.3049 - accuracy: 0.7038 - val_loss: 2.5671 - val_perplexity: 13.0278 - val_accuracy: 0.6988 - lr: 0.0010\nEpoch 4/20\n333/333 [==============================] - 156s 469ms/step - loss: 2.0528 - perplexity: 7.7897 - accuracy: 0.7115 - val_loss: 2.5974 - val_perplexity: 13.4293 - val_accuracy: 0.6990 - lr: 0.0010\nEpoch 5/20\n333/333 [==============================] - 156s 469ms/step - loss: 1.8858 - perplexity: 6.5918 - accuracy: 0.7221 - val_loss: 2.6526 - val_perplexity: 14.1908 - val_accuracy: 0.6982 - lr: 0.0010\nEpoch 6/20\n333/333 [==============================] - 156s 468ms/step - loss: 1.7402 - perplexity: 5.6982 - accuracy: 0.7314 - val_loss: 2.7387 - val_perplexity: 15.4673 - val_accuracy: 0.6974 - lr: 0.0010\nEpoch 7/20\n333/333 [==============================] - 154s 461ms/step - loss: 1.6236 - perplexity: 5.0713 - accuracy: 0.7378 - val_loss: 2.8240 - val_perplexity: 16.8437 - val_accuracy: 0.6962 - lr: 0.0010\nEpoch 8/20\n333/333 [==============================] - 156s 469ms/step - loss: 1.5346 - perplexity: 4.6394 - accuracy: 0.7436 - val_loss: 2.9181 - val_perplexity: 18.5069 - val_accuracy: 0.6897 - lr: 0.0010\nEpoch 9/20\n333/333 [==============================] - 156s 470ms/step - loss: 1.4583 - perplexity: 4.2986 - accuracy: 0.7495 - val_loss: 2.9847 - val_perplexity: 19.7809 - val_accuracy: 0.6921 - lr: 0.0010\nEpoch 9: early stopping\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7a032001b220>"},"metadata":{}}]},{"cell_type":"code","source":"def predict_word(input_text):\n  predicted_words = []\n  #print(input_text)\n  tokenized_prompt = vectorize([input_text])[:, :-1]\n  predictions = model.predict([tokenized_prompt], verbose=0)\n  sample_index = len(input_text.strip().split())-1\n  #print(predictions[0][sample_index])\n  logits, indices = tf.math.top_k(predictions[0][sample_index], k=20, sorted=True)\n  indices = np.asarray(indices).astype(\"int32\")\n  #print(indices)\n  for i in indices:\n    predicted_words.append(vocab_list[i])\n\n  return predicted_words\n","metadata":{"id":"xrPsr4ynmvEJ","execution":{"iopub.status.busy":"2023-08-16T13:14:16.931406Z","iopub.execute_input":"2023-08-16T13:14:16.931815Z","iopub.status.idle":"2023-08-16T13:14:16.944880Z","shell.execute_reply.started":"2023-08-16T13:14:16.931777Z","shell.execute_reply":"2023-08-16T13:14:16.943776Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def autocorrect_sentence(sentence):\n  new_sentence = \" \"\n  mistake = 0\n  wrong_word = \"\"\n  tmp_predict = dict()\n  tmp_word=[]\n  tmp_key=[]\n  correct_word = \"\"\n  words = sentence.lower()\n  words = sentence.split()\n  for word in words:\n    if word in dictionary:\n      new_sentence += word\n      new_sentence += \" \"\n    else:\n      wrong_word = word\n      mistake = 1\n      break\n\n  if mistake == 0:\n    print(\"The final corrected sentence is: \", new_sentence)\n    #return new_sentence\n  else:\n  #if len(new_sentence) < len(sentence):\n    predicted_words = predict_word(new_sentence)\n    min_distance = 100\n    for word in predicted_words:\n      distance = edit_distance(word, wrong_word)\n      #print(word,\" : \",distance)\n      tmp_key.append(distance)\n      tmp_word.append(word)\n      \"\"\"\n      predict_dict = sorted(predict_dict)\n      print(predict_dict)\n      if distance < min_distance:\n        correct_word = word\n        min_distance = distance\n    \"\"\"\n\n    tmp_predict = dict(zip(tmp_word,tmp_key))\n    keys = list(tmp_predict.keys())\n    values = list(tmp_predict.values())\n    sorted_value_index = np.argsort(values)\n    tmp_predict = {keys[i]: values[i] for i in sorted_value_index}\n\n    #print(tmp_predict)\n    for word in tmp_predict.keys():\n      if len(word)>3 and (word[0] == wrong_word.lower()[0] and word[1] == wrong_word.lower()[1]) :\n        correct_word = word\n        new_sentence += correct_word\n\n    print(\"Wrong word: \",wrong_word,\" Correct word\", correct_word)\n    #print(new_sentence)\n    #print(len(words))\n\n    new_sentence_words = new_sentence.split()\n    for i in range(len(new_sentence_words),len(words)):\n      new_sentence += \" \"\n      new_sentence += words[i]\n    #print(new_sentence)\n\n\n    autocorrect_sentence(new_sentence)\n\n\n","metadata":{"id":"BEYifyYtnQG_","execution":{"iopub.status.busy":"2023-08-16T13:20:51.995848Z","iopub.execute_input":"2023-08-16T13:20:51.997054Z","iopub.status.idle":"2023-08-16T13:20:52.010281Z","shell.execute_reply.started":"2023-08-16T13:20:51.997003Z","shell.execute_reply":"2023-08-16T13:20:52.009098Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"texts = [\"i am extremely ANSIOUS about the matter how it will apper to them\",\n        \"i have been thinking of all the foreastd areas\",\n        \"i am going away to meet my wift\",\n        \"where have you been all this while\"]\n\nfor text in texts:\n  print(\"\\n\\nInput Sentence:\", text.lower())\n  output = autocorrect_sentence(text)\n  print(output)","metadata":{"id":"tDp870lpp_Rd","execution":{"iopub.status.busy":"2023-08-16T13:20:54.922662Z","iopub.execute_input":"2023-08-16T13:20:54.923056Z","iopub.status.idle":"2023-08-16T13:20:55.275106Z","shell.execute_reply.started":"2023-08-16T13:20:54.923025Z","shell.execute_reply":"2023-08-16T13:20:55.274152Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\n\nInput Sentence: i am extremely ansious about the matter how it will apper to them\nWrong word:  ANSIOUS  Correct word anxious\nWrong word:  apper  Correct word appear\nThe final corrected sentence is:   i am extremely anxious about the matter how it will appear to them \nNone\n\n\nInput Sentence: i have been thinking of all the foreastd areas\nWrong word:  foreastd  Correct word forested\nThe final corrected sentence is:   i have been thinking of all the forested areas \nNone\n\n\nInput Sentence: i am going away to meet my wift\nWrong word:  wift  Correct word wife\nThe final corrected sentence is:   i am going away to meet my wife \nNone\n\n\nInput Sentence: where have you been all this while\nThe final corrected sentence is:   where have you been all this while \nNone\n","output_type":"stream"}]}]}